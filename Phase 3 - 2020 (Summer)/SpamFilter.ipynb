{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmTrain(X, Y, C, kernelFunction, tol=1e-3, max_passes=5, args=()):\n",
    "    \"\"\"\n",
    "    Trains an SVM classifier using a  simplified version of the SMO algorithm.\n",
    "    Parameters\n",
    "    ---------\n",
    "    X : numpy ndarray\n",
    "        (m x n) Matrix of training examples. Each row is a training example, and the\n",
    "        jth column holds the jth feature.\n",
    "    Y : numpy ndarray\n",
    "        (m, ) A vector (1-D numpy array) containing 1 for positive examples and 0 for negative examples.\n",
    "    C : float\n",
    "        The standard SVM regularization parameter.\n",
    "    kernelFunction : func\n",
    "        A function handle which computes the kernel. The function should accept two vectors as\n",
    "        inputs, and returns a scalar as output.\n",
    "    tol : float, optional\n",
    "        Tolerance value used for determining equality of floating point numbers.\n",
    "    max_passes : int, optional\n",
    "        Controls the number of iterations over the dataset (without changes to alpha)\n",
    "        before the algorithm quits.\n",
    "    args : tuple\n",
    "        Extra arguments required for the kernel function, such as the sigma parameter for a\n",
    "        Gaussian kernel.\n",
    "    Returns\n",
    "    -------\n",
    "    model :\n",
    "        The trained SVM model.\n",
    "    Notes\n",
    "    -----\n",
    "    This is a simplified version of the SMO algorithm for training SVMs. In practice, if\n",
    "    you want to train an SVM classifier, we recommend using an optimized package such as:\n",
    "    - LIBSVM   (http://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n",
    "    - SVMLight (http://svmlight.joachims.org/)\n",
    "    - scikit-learn (http://scikit-learn.org/stable/modules/svm.html) which contains python wrappers\n",
    "    for the LIBSVM library.\n",
    "    \"\"\"\n",
    "    # make sure data is signed int\n",
    "    Y = Y.astype(int)\n",
    "    # Dataset size parameters\n",
    "    m, n = X.shape\n",
    "\n",
    "    passes = 0\n",
    "    E = np.zeros(m)\n",
    "    alphas = np.zeros(m)\n",
    "    b = 0\n",
    "\n",
    "    # Map 0 to -1\n",
    "    Y[Y == 0] = -1\n",
    "\n",
    "    # Pre-compute the Kernel Matrix since our dataset is small\n",
    "    # (in practice, optimized SVM packages that handle large datasets\n",
    "    # gracefully will **not** do this)\n",
    "\n",
    "    # We have implemented the optimized vectorized version of the Kernels here so\n",
    "    # that the SVM training will run faster\n",
    "    if kernelFunction.__name__ == 'linearKernel':\n",
    "        # Vectorized computation for the linear kernel\n",
    "        # This is equivalent to computing the kernel on every pair of examples\n",
    "        K = np.dot(X, X.T)\n",
    "    elif kernelFunction.__name__ == 'gaussianKernel':\n",
    "        # vectorized RBF Kernel\n",
    "        # This is equivalent to computing the kernel on every pair of examples\n",
    "        X2 = np.sum(X**2, axis=1)\n",
    "        K = X2 + X2[:, None] - 2 * np.dot(X, X.T)\n",
    "\n",
    "        if len(args) > 0:\n",
    "            K /= 2*args[0]**2\n",
    "\n",
    "        K = np.exp(-K)\n",
    "    else:\n",
    "        K = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            for j in range(i, m):\n",
    "                K[i, j] = kernelFunction(X[i, :], X[j, :])\n",
    "                K[j, i] = K[i, j]\n",
    "\n",
    "    while passes < max_passes:\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(m):\n",
    "            E[i] = b + np.sum(alphas * Y * K[:, i]) - Y[i]\n",
    "\n",
    "            if (Y[i]*E[i] < -tol and alphas[i] < C) or (Y[i]*E[i] > tol and alphas[i] > 0):\n",
    "                # select the alpha_j randomly\n",
    "                j = np.random.choice(list(range(i)) + list(range(i+1, m)), size=1)[0]\n",
    "\n",
    "                E[j] = b + np.sum(alphas * Y * K[:, j]) - Y[j]\n",
    "\n",
    "                alpha_i_old = alphas[i]\n",
    "                alpha_j_old = alphas[j]\n",
    "\n",
    "                if Y[i] == Y[j]:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "\n",
    "                if L == H:\n",
    "                    continue\n",
    "\n",
    "                eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
    "\n",
    "                # objective function positive definite, there will be a minimum along the direction\n",
    "                # of linear equality constrain, and eta will be greater than zero\n",
    "                # we are actually computing -eta here (so we skip of eta >= 0)\n",
    "                if eta >= 0:\n",
    "                    continue\n",
    "\n",
    "                alphas[j] -= Y[j] * (E[i] - E[j])/eta\n",
    "                alphas[j] = max(L, min(H, alphas[j]))\n",
    "\n",
    "                if abs(alphas[j] - alpha_j_old) < tol:\n",
    "                    alphas[j] = alpha_j_old\n",
    "                    continue\n",
    "                alphas[i] += Y[i]*Y[j]*(alpha_j_old - alphas[j])\n",
    "\n",
    "                b1 = b - E[i] - Y[i]*(alphas[i] - alpha_i_old) * K[i, j] \\\n",
    "                     - Y[j] * (alphas[j] - alpha_j_old) * K[i, j]\n",
    "\n",
    "                b2 = b - E[j] - Y[i]*(alphas[i] - alpha_i_old) * K[i, j] \\\n",
    "                     - Y[j] * (alphas[j] - alpha_j_old) * K[j, j]\n",
    "\n",
    "                if 0 < alphas[i] < C:\n",
    "                    b = b1\n",
    "                elif 0 < alphas[j] < C:\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1 + b2)/2\n",
    "\n",
    "                num_changed_alphas += 1\n",
    "        if num_changed_alphas == 0:\n",
    "            passes += 1\n",
    "        else:\n",
    "            passes = 0\n",
    "\n",
    "    idx = alphas > 0\n",
    "    model = {'X': X[idx, :],\n",
    "             'y': Y[idx],\n",
    "             'kernelFunction': kernelFunction,\n",
    "             'b': b,\n",
    "             'args': args,\n",
    "             'alphas': alphas[idx],\n",
    "             'w': np.dot(alphas * Y, X)}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmPredict(model, X):\n",
    "    \"\"\"\n",
    "    Returns a vector of predictions using a trained SVM model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict\n",
    "        The parameters of the trained svm model, as returned by the function svmTrain\n",
    "    X : array_like\n",
    "        A (m x n) matrix where each example is a row.\n",
    "    Returns\n",
    "    -------\n",
    "    pred : array_like\n",
    "        A (m,) sized vector of predictions {0, 1} values.\n",
    "    \"\"\"\n",
    "    # check if we are getting a vector. If so, then assume we only need to do predictions\n",
    "    # for a single example\n",
    "    if X.ndim == 1:\n",
    "        X = X[np.newaxis, :]\n",
    "\n",
    "    m = X.shape[0]\n",
    "    p = np.zeros(m)\n",
    "    pred = np.zeros(m)\n",
    "\n",
    "    if model['kernelFunction'].__name__ == 'linearKernel':\n",
    "        # we can use the weights and bias directly if working with the linear kernel\n",
    "        p = np.dot(X, model['w']) + model['b']\n",
    "    elif model['kernelFunction'].__name__ == 'gaussianKernel':\n",
    "        # vectorized RBF Kernel\n",
    "        # This is equivalent to computing the kernel on every pair of examples\n",
    "        X1 = np.sum(X**2, 1)\n",
    "        X2 = np.sum(model['X']**2, 1)\n",
    "        K = X2 + X1[:, None] - 2 * np.dot(X, model['X'].T)\n",
    "\n",
    "        if len(model['args']) > 0:\n",
    "            K /= 2*model['args'][0]**2\n",
    "\n",
    "        K = np.exp(-K)\n",
    "        p = np.dot(K, model['alphas']*model['y']) + model['b']\n",
    "    else:\n",
    "        # other non-linear kernel\n",
    "        for i in range(m):\n",
    "            predictions = 0\n",
    "            for j in range(model['X'].shape[0]):\n",
    "                predictions += model['alphas'][j] * model['y'][j] \\\n",
    "                               * model['kernelFunction'](X[i, :], model['X'][j, :])\n",
    "            p[i] = predictions\n",
    "\n",
    "    pred[p >= 0] = 1\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearKernel(x1, x2):\n",
    "    \"\"\"\n",
    "    Returns a linear kernel between x1 and x2.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : numpy ndarray\n",
    "        A 1-D vector.\n",
    "    x2 : numpy ndarray\n",
    "        A 1-D vector of same size as x1.\n",
    "    Returns\n",
    "    -------\n",
    "    : float\n",
    "        The scalar amplitude.\n",
    "    \"\"\"\n",
    "    return np.dot(x1, x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabList():\n",
    "    \"\"\"\n",
    "    Reads the fixed vocabulary list in vocab.txt and returns a cell array of the words\n",
    "    %   vocabList = GETVOCABLIST() reads the fixed vocabulary list in vocab.txt\n",
    "    %   and returns a cell array of the words in vocabList.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vocabList = np.genfromtxt('vocab.txt', dtype=object)\n",
    "    return list(vocabList[:, 1].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PorterStemmer:\n",
    "    \"\"\"\n",
    "    Porter Stemming Algorithm\n",
    "    This is the Porter stemming algorithm, ported to Python from the\n",
    "    version coded up in ANSI C by the author. It may be be regarded\n",
    "    as canonical, in that it follows the algorithm presented in\n",
    "    Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14,\n",
    "    no. 3, pp 130-137,\n",
    "    only differing from it at the points maked --DEPARTURE-- below.\n",
    "    See also http://www.tartarus.org/~martin/PorterStemmer\n",
    "    The algorithm as described in the paper could be exactly replicated\n",
    "    by adjusting the points of DEPARTURE, but this is barely necessary,\n",
    "    because (a) the points of DEPARTURE are definitely improvements, and\n",
    "    (b) no encoding of the Porter stemmer I have seen is anything like\n",
    "    as exact as this version, even with the points of DEPARTURE!\n",
    "    Vivake Gupta (v@nano.com)\n",
    "    Release 1: January 2001\n",
    "    Further adjustments by Santiago Bruno (bananabruno@gmail.com)\n",
    "    to allow word input not restricted to one word per line, leading\n",
    "    to:\n",
    "    release 2: July 2008\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The main part of the stemming algorithm starts here.\n",
    "        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n",
    "        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n",
    "        readjusted downwards as the stemming progresses. Zero termination is\n",
    "        not in fact used in the algorithm.\n",
    "        Note that only lower case sequences are stemmed. Forcing to lower case\n",
    "        should be done before stem(...) is called.\n",
    "        \"\"\"\n",
    "        self.b = \"\"  # buffer for word to be stemmed\n",
    "        self.k = 0\n",
    "        self.k0 = 0\n",
    "        self.j = 0   # j is a general offset into the string\n",
    "\n",
    "    def cons(self, i):\n",
    "        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n",
    "        if self.b[i] in 'aeiou':\n",
    "            return 0\n",
    "        if self.b[i] == 'y':\n",
    "            if i == self.k0:\n",
    "                return 1\n",
    "            else:\n",
    "                return not self.cons(i - 1)\n",
    "        return 1\n",
    "\n",
    "    def m(self):\n",
    "        \"\"\"\n",
    "        m() measures the number of consonant sequences between k0 and j.\n",
    "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
    "        indicates arbitrary presence,\n",
    "           <c><v>       gives 0\n",
    "           <c>vc<v>     gives 1\n",
    "           <c>vcvc<v>   gives 2\n",
    "           <c>vcvcvc<v> gives 3\n",
    "           ....\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        i = self.k0\n",
    "        while 1:\n",
    "            if i > self.j:\n",
    "                return n\n",
    "            if not self.cons(i):\n",
    "                break\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "        while 1:\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "            n = n + 1\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if not self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "\n",
    "    def vowelinstem(self):\n",
    "        \"\"\"vowelinstem() is TRUE <=> k0,...j contains a vowel\"\"\"\n",
    "        for i in range(self.k0, self.j + 1):\n",
    "            if not self.cons(i):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def doublec(self, j):\n",
    "        \"\"\" doublec(j) is TRUE <=> j,(j-1) contain a double consonant. \"\"\"\n",
    "        if j < (self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[j] != self.b[j-1]:\n",
    "            return 0\n",
    "        return self.cons(j)\n",
    "\n",
    "    def cvc(self, i):\n",
    "        \"\"\"\n",
    "        cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n",
    "        and also if the second c is not w,x or y. this is used when trying to\n",
    "        restore an e at the end of a short  e.g.\n",
    "           cav(e), lov(e), hop(e), crim(e), but\n",
    "           snow, box, tray.\n",
    "        \"\"\"\n",
    "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
    "            return 0\n",
    "        ch = self.b[i]\n",
    "        if ch in 'wxy':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends(self, s):\n",
    "        \"\"\"ends(s) is TRUE <=> k0,...k ends with the string s.\"\"\"\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
    "            return 0\n",
    "        if length > (self.k - self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[self.k-length+1:self.k+1] != s:\n",
    "            return 0\n",
    "        self.j = self.k - length\n",
    "        return 1\n",
    "\n",
    "    def setto(self, s):\n",
    "        \"\"\"setto(s) sets (j+1),...k to the characters in the string s, readjusting k.\"\"\"\n",
    "        length = len(s)\n",
    "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
    "        self.k = self.j + length\n",
    "\n",
    "    def r(self, s):\n",
    "        \"\"\"r(s) is used further down.\"\"\"\n",
    "        if self.m() > 0:\n",
    "            self.setto(s)\n",
    "\n",
    "    def step1ab(self):\n",
    "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
    "           caresses  ->  caress\n",
    "           ponies    ->  poni\n",
    "           ties      ->  ti\n",
    "           caress    ->  caress\n",
    "           cats      ->  cat\n",
    "           feed      ->  feed\n",
    "           agreed    ->  agree\n",
    "           disabled  ->  disable\n",
    "           matting   ->  mat\n",
    "           mating    ->  mate\n",
    "           meeting   ->  meet\n",
    "           milling   ->  mill\n",
    "           messing   ->  mess\n",
    "           meetings  ->  meet\n",
    "        \"\"\"\n",
    "        if self.b[self.k] == 's':\n",
    "            if self.ends(\"sses\"):\n",
    "                self.k = self.k - 2\n",
    "            elif self.ends(\"ies\"):\n",
    "                self.setto(\"i\")\n",
    "            elif self.b[self.k - 1] != 's':\n",
    "                self.k = self.k - 1\n",
    "        if self.ends(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.k = self.k - 1\n",
    "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
    "            self.k = self.j\n",
    "            if self.ends(\"at\"):\n",
    "                self.setto(\"ate\")\n",
    "            elif self.ends(\"bl\"):\n",
    "                self.setto(\"ble\")\n",
    "            elif self.ends(\"iz\"):\n",
    "                self.setto(\"ize\")\n",
    "            elif self.doublec(self.k):\n",
    "                self.k = self.k - 1\n",
    "                ch = self.b[self.k]\n",
    "                if ch in 'lsz':\n",
    "                    self.k += 1\n",
    "            elif self.m() == 1 and self.cvc(self.k):\n",
    "                self.setto(\"e\")\n",
    "\n",
    "    def step1c(self):\n",
    "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
    "        if self.ends(\"y\") and self.vowelinstem():\n",
    "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
    "\n",
    "    def step2(self):\n",
    "        \"\"\"step2() maps double suffices to single ones.\n",
    "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
    "        string before the suffix must give m() > 0.\n",
    "        \"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
    "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
    "            # To match the published algorithm, replace this phrase with\n",
    "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
    "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
    "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
    "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
    "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
    "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"alism\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
    "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
    "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
    "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
    "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
    "            if self.ends(\"logi\"):      self.r(\"log\")\n",
    "        # To match the published algorithm, delete this phrase\n",
    "\n",
    "    def step3(self):\n",
    "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
    "        if self.b[self.k] == 'e':\n",
    "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
    "            elif self.ends(\"ative\"):   self.r(\"\")\n",
    "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
    "        elif self.b[self.k] == 'i':\n",
    "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
    "        elif self.b[self.k] == 'l':\n",
    "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
    "            elif self.ends(\"ful\"):     self.r(\"\")\n",
    "        elif self.b[self.k] == 's':\n",
    "            if self.ends(\"ness\"):      self.r(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"al\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"ance\"): pass\n",
    "            elif self.ends(\"ence\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"er\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'i':\n",
    "            if self.ends(\"ic\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"able\"): pass\n",
    "            elif self.ends(\"ible\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'n':\n",
    "            if self.ends(\"ant\"): pass\n",
    "            elif self.ends(\"ement\"): pass\n",
    "            elif self.ends(\"ment\"): pass\n",
    "            elif self.ends(\"ent\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
    "            elif self.ends(\"ou\"): pass\n",
    "            # takes care of -ous\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"ism\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"ate\"): pass\n",
    "            elif self.ends(\"iti\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'u':\n",
    "            if self.ends(\"ous\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'v':\n",
    "            if self.ends(\"ive\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'z':\n",
    "            if self.ends(\"ize\"): pass\n",
    "            else: return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.k = self.j\n",
    "\n",
    "    def step5(self):\n",
    "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
    "        m() > 1.\n",
    "        \"\"\"\n",
    "        self.j = self.k\n",
    "        if self.b[self.k] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
    "                self.k = self.k - 1\n",
    "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
    "            self.k = self.k -1\n",
    "\n",
    "    def stem(self, p, i=0, j=None):\n",
    "        \"\"\"In stem(p,i,j), p is a char pointer, and the string to be stemmed\n",
    "        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n",
    "        offset to the last character of a string, (p[j+1] == '\\0'). The\n",
    "        stemmer adjusts the characters p[i] ... p[j] and returns the new\n",
    "        end-point of the string, k. Stemming never increases word length, so\n",
    "        i <= k <= j. To turn the stemmer into a module, declare 'stem' as\n",
    "        extern, and delete the remainder of this file.\n",
    "        \"\"\"\n",
    "        # copy the parameters into statics\n",
    "        self.b = p\n",
    "        self.k = j or len(p) - 1\n",
    "        self.k0 = i\n",
    "        if self.k <= self.k0 + 1:\n",
    "            return self.b  # --DEPARTURE--\n",
    "\n",
    "        # With this line, strings of length 1 or 2 don't go through the\n",
    "        # stemming process, although no mention is made of this in the\n",
    "        # published algorithm. Remove the line to match the published\n",
    "        # algorithm.\n",
    "\n",
    "        self.step1ab()\n",
    "        self.step1c()\n",
    "        self.step2()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.b[self.k0:self.k+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('emailSample1.txt')) as fid:\n",
    "    file_contents = fid.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmail(email_contents, verbose=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the body of an email and returns a list of indices \n",
    "    of the words contained in the email.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    email_contents : str\n",
    "        A string containing one email. \n",
    "    \n",
    "    verbose : bool\n",
    "        If True, print the resulting email after processing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word_indices : list\n",
    "        A list of integers containing the index of each word in the \n",
    "        email which is also present in the vocabulary.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to add the index of word to word_indices \n",
    "    if it is in the vocabulary. At this point of the code, you have \n",
    "    a stemmed word from the email in the variable word.\n",
    "    You should look up word in the vocabulary list (vocabList). \n",
    "    If a match exists, you should add the index of the word to the word_indices\n",
    "    list. Concretely, if word = 'action', then you should\n",
    "    look up the vocabulary list to find where in vocabList\n",
    "    'action' appears. For example, if vocabList[18] =\n",
    "    'action', then, you should add 18 to the word_indices \n",
    "    vector (e.g., word_indices.append(18)).\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - vocabList[idx] returns a the word with index idx in the vocabulary list.\n",
    "    \n",
    "    - vocabList.index(word) return index of word `word` in the vocabulary list.\n",
    "      (A ValueError exception is raised if the word does not exist.)\n",
    "    \"\"\"\n",
    "    # Load Vocabulary\n",
    "    vocabList = getVocabList()\n",
    "\n",
    "    # Init return value\n",
    "    word_indices = []\n",
    "\n",
    "    # ========================== Preprocess Email ===========================\n",
    "    # Find the Headers ( \\n\\n and remove )\n",
    "    # Uncomment the following lines if you are working with raw emails with the\n",
    "    # full headers\n",
    "    # hdrstart = email_contents.find(chr(10) + chr(10))\n",
    "    # email_contents = email_contents[hdrstart:]\n",
    "\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # and does not have any < or > in the tag it with a space\n",
    "    email_contents =re.compile('<[^<>]+>').sub(' ', email_contents)\n",
    "\n",
    "    # Handle Numbers\n",
    "    # Look for one or more characters between 0-9\n",
    "    email_contents = re.compile('[0-9]+').sub(' number ', email_contents)\n",
    "\n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https://\n",
    "    email_contents = re.compile('(http|https)://[^\\s]*').sub(' httpaddr ', email_contents)\n",
    "\n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle\n",
    "    email_contents = re.compile('[^\\s]+@[^\\s]+').sub(' emailaddr ', email_contents)\n",
    "    \n",
    "    # Handle $ sign\n",
    "    email_contents = re.compile('[$]+').sub(' dollar ', email_contents)\n",
    "    \n",
    "    # get rid of any punctuation\n",
    "    email_contents = re.split('[ @$/#.-:&*+=\\[\\]?!(){},''\">_<;%\\n\\r]', email_contents)\n",
    "\n",
    "    # remove any empty word string\n",
    "    email_contents = [word for word in email_contents if len(word) > 0]\n",
    "    \n",
    "    # Stem the email contents word by word\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_email = []\n",
    "    for word in email_contents:\n",
    "        # Remove any remaining non alphanumeric characters in word\n",
    "        word = re.compile('[^a-zA-Z0-9]').sub('', word).strip()\n",
    "        word = stemmer.stem(word)\n",
    "        processed_email.append(word)\n",
    "\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        try:\n",
    "            word_indices.append(vocabList.index(word))\n",
    "        except ValueError:\n",
    "                pass\n",
    "    if verbose:\n",
    "        print('----------------')\n",
    "        print('Processed email:')\n",
    "        print('----------------')\n",
    "        print(' '.join(processed_email))\n",
    "    return word_indices\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Processed email:\n",
      "----------------\n",
      "anyon know how much it cost to host a web portal well it depend on how mani visitor your expect thi can be anywher from less than number buck a month to a coupl of dollar number you should checkout httpaddr or perhap amazon ec number if your run someth big to unsubscrib yourself from thi mail list send an email to emailaddr\n",
      "-------------\n",
      "Word Indices:\n",
      "-------------\n",
      "[85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 882, 430, 1170, 793, 1001, 1894, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 1698, 374, 1161, 476, 1119, 1892, 1509, 798, 1181, 1236, 511, 1119, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 687, 1675, 991, 960, 1476, 70, 529, 1698, 530]\n"
     ]
    }
   ],
   "source": [
    "word_indices  = processEmail(file_contents)\n",
    "print('-------------')\n",
    "print('Word Indices:')\n",
    "print('-------------')\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "    \"\"\"\n",
    "    Takes in a word_indices vector and produces a feature vector from the word indices. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_indices : list\n",
    "        A list of word indices from the vocabulary list.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : list \n",
    "        The computed feature vector.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return a feature vector for the\n",
    "    given email (word_indices). To help make it easier to  process \n",
    "    the emails, we have have already pre-processed each email and converted\n",
    "    each word in the email into an index in a fixed dictionary (of 1899 words).\n",
    "    The variable `word_indices` contains the list of indices of the words \n",
    "    which occur in one email.\n",
    "    \n",
    "    Concretely, if an email has the text:\n",
    "\n",
    "        The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "    Then, the word_indices vector for this text might look  like:\n",
    "               \n",
    "        60  100   33   44   10     53  60  58   5\n",
    "\n",
    "    where, we have mapped each word onto a number, for example:\n",
    "\n",
    "        the   -- 60\n",
    "        quick -- 100\n",
    "        ...\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The above numbers are just an example and are not the actual mappings.\n",
    "\n",
    "    Your task is take one such `word_indices` vector and construct\n",
    "    a binary feature vector that indicates whether a particular\n",
    "    word occurs in the email. That is, x[i] = 1 when word i\n",
    "    is present in the email. Concretely, if the word 'the' (say,\n",
    "    index 60) appears in the email, then x[60] = 1. The feature\n",
    "    vector should look like:\n",
    "        x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..]\n",
    "    \"\"\"\n",
    "    # Total number of words in the dictionary\n",
    "    n = 1899\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = np.zeros(n)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "\n",
    "    x[word_indices] = 1\n",
    "    \n",
    "    # ===========================================================\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Processed email:\n",
      "----------------\n",
      "anyon know how much it cost to host a web portal well it depend on how mani visitor your expect thi can be anywher from less than number buck a month to a coupl of dollar number you should checkout httpaddr or perhap amazon ec number if your run someth big to unsubscrib yourself from thi mail list send an email to emailaddr\n",
      "\n",
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: 45\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('emailSample1.txt')) as fid:\n",
    "    file_contents = fid.read()\n",
    "\n",
    "word_indices  = processEmail(file_contents)\n",
    "features      = emailFeatures(word_indices)\n",
    "\n",
    "# Print Stats\n",
    "print('\\nLength of feature vector: %d' % len(features))\n",
    "print('Number of non-zero entries: %d' % sum(features > 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear SVM (Spam Classification)\n",
      "This may take 1 to 2 minutes ...\\n\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(os.path.join('spamTrain.mat'))\n",
    "X, y= data['X'].astype(float), data['y'][:, 0]\n",
    "print('Training Linear SVM (Spam Classification)')\n",
    "print('This may take 1 to 2 minutes ...\\\\n')\n",
    "C = 0.1\n",
    "model = svmTrain(X, y, C, linearKernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.83\n"
     ]
    }
   ],
   "source": [
    "p = svmPredict(model, X)\n",
    "print('Training Accuracy: %.2f' % (np.mean(p == y) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the trained Linear SVM on a test set ...\n",
      "Test Accuracy: 98.80\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(os.path.join('spamTest.mat'))\n",
    "Xtest, ytest = data['Xtest'].astype(float), data['ytest'][:, 0]\n",
    "print('Evaluating the trained Linear SVM on a test set ...')\n",
    "p = svmPredict(model, Xtest)\n",
    "print('Test Accuracy: %.2f' % (np.mean(p == ytest) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictors of spam:\n",
      "word            weight         \n",
      "----            ------\n",
      "our             0.50\n",
      "click           0.47\n",
      "remov           0.42\n",
      "guarante        0.38\n",
      "visit           0.37\n",
      "basenumb        0.34\n",
      "dollar          0.33\n",
      "will            0.27\n",
      "pleas           0.26\n",
      "price           0.26\n",
      "most            0.26\n",
      "nbsp            0.25\n",
      "lo              0.25\n",
      "ga              0.24\n",
      "da              0.24\n"
     ]
    }
   ],
   "source": [
    "idx = np.argsort(model['w'])\n",
    "top_idx = idx[-15:][::-1]\n",
    "vocabList = getVocabList()\n",
    "print('Top predictors of spam:')\n",
    "print('%-15s %-15s' % ('word', 'weight'))\n",
    "print('----' + ' '*12 + '------')\n",
    "for word, w in zip(np.array(vocabList)[top_idx], model['w'][top_idx]):\n",
    "    print('%-15s %0.2f' % (word, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nProcessed emailSample1.txt\\nSpam Classification: not spam\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join('emailSample1.txt')\n",
    "with open(filename) as fid:\n",
    "    file_contents = fid.read()\n",
    "    word_indices = processEmail(file_contents, verbose=False)\n",
    "    x = emailFeatures(word_indices)\n",
    "    p = svmPredict(model, x)\n",
    "    print('\\\\nProcessed %s\\\\nSpam Classification: %s' % (filename, 'spam' if p else 'not spam'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
